Predicting Constructive Listening: A Multimodal Analysis of Empathy in Opposing View Dialogues

I. Abstract
 In an era of online polarization and ideological echo chambers, constructive listening has become increasingly rare. People often engage in debates not to understand but to defend their beliefs, leading to misunderstanding, mischaracterization, and conflict. This study proposes the development of an Opposing View Constructive Listening Prediction Model, an AI-assisted framework designed to identify behavioral and verbal predictors of empathetic listening. Participants will engage in spoken conversations with customized ChatGPT agents configured to adopt varying stances and tones on sensitive topics such as politics, religion, and social issues. Using multimodal data from participants’ facial expression, vocal tone, and verbal patterns, the study will evaluate how accurately participants can summarize the AI’s viewpoint after each dialogue. Statistical and neural network analyses will then determine which emotional and cognitive features correlate with higher or lower listening accuracy. The findings aim to reveal measurable indicators of constructive listening and provide the foundation for empathy-focused training tools and AI systems that promote understanding rather than division. 

II. Introduction
In today’s digital world, discussions on social and political issues often devolve into conflict rather than understanding. Social media algorithms and ideological echo chambers reinforce selective exposure and motivated reasoning, where individuals unconsciously seek information that validates their beliefs while dismissing opposing views. As a result, people frequently talk past each other, misinterpret arguments, and fail to engage in constructive listening which is a key component of empathy and civilized discourse.

This study addresses the problem of how people process and respond to opposing viewpoints during conversations, and whether measurable behavioral or linguistic cues can indicate genuine listening. By combining insights from psychology, communication studies, and artificial intelligence, this research aims to quantify empathy through observable signals such as facial expression, vocal tone, and verbal patterns.

Research Question: 
Which multimodal features: facial, vocal, and linguistic can predict an individual’s ability to constructively listen and accurately summarize an opposing viewpoint?

Hypothesis: 
Participants exhibiting calmer vocal tones, balanced facial expressions, and validating word choices will demonstrate higher listening accuracy and empathy levels

Intended Contribution: 
This study seeks to create a predictive framework for constructive listening that can inform empathy-training programs for chat bots and other social technologies that encourage understanding over division. 
IV. Methodology
 Experimental Design 
● Independent Variables (IVs):
○ Topic stance of ChatGPT (pro, anti, or neutral) 
○ Pre-loaded data provided to ChatGPT 
○ Tone setting of ChatGPT (friendly, neutral, confrontational) 
● Dependent Variables (DVs): 
○ Participants’ emotional signals 
○ Emotional signals (facial expression intensity and valence) 
○ Vocal tone and verbal patterns 
○ Listening accuracy (measured by the clarity and correctness of participant summaries) 
● Controls:
 ○ Same recording setup (lighting, camera angle, white background) 
○ Equal conversation length (10-15 minutes) 
○ Standardized ChatGPT prompts and structure 
● Confounds to Monitor: 
○ Differences in speaking ability, emotional regulation, or prior familiarity with AI systems
○ Prior mood, fatigue, and time of day of testing 
○ Topic sensitivity or emotional distress during testing 

Prototype or Dataset Plan
● Develop a custom ChatGPT-based conversational prototype configured with factual data and preset tones for contentious topics (e.g., politics, religion, LGBT rights). 
● Visual data will be collected through recorded video and audio during testing 
● Textual data will be collected from the transcripts from ChatGPT’s records 
● Collected video will be processed into an emotional timeline through Python libraries for analysis. 

Participant Plan 
● Recruit 15-20 participants through: 
○ The LGBTQ+ student organization (PRISM) (for varied perspectives on social issues)
○ The Debate Society (experienced in argumentation and critical discussion) 
○ Center for Human-Computer Innovations (CeHCI) Lab (for participants familiar with human-AI interaction).
● Inclusion criteria: English proficiency, willingness to engage in sensitive discussions, and consent for video/audio recording. 
● Participants will be randomly assigned to control or experimental conditions. The topic selection will be based on questionnaire inputs. 

Data Collection and Annotation Plan 
● Conduct sessions over Zoom recorded using OBS and Zoom’s cloud recording. 
● Each session: 
○ 10–15 minutes of spoken interaction with ChatGPT via microphone.
○ Post-conversation summary task where the participant lists ChatGPT’s main points and restates its stance. 
○ Data will be collected via ChatGPT’s share chat system 
● Annotate data for: 
○ Facial expressions 
○ Vocal features and word use 
○ Text summaries

Evaluation Metrics
● Listening Index combining: 
○ Summary accuracy score 
○ Emotion stability (low volatility in emotion graphs) 
○ Vocal tone consistency 
○ Politeness and validation words per minute 
● Statistical Analysis: 
○ Compare means between groups (e.g., friendly vs. confrontational bot tone). 
○ Correlation and regression to identify predictors of constructive listening. 
● Neural Network (Optional/To be determined): 
○ Use a neural network training model to cross-validate responses and values to predict listening on raw video. 
